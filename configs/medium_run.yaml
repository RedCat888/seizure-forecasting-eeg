# Medium run configuration for more robust training
# Uses 10-15 subjects, more epochs, regularization, augmentation

# =============================================================================
# DATA PATHS
# =============================================================================
data:
  raw_root: "data/chbmit_raw"
  cache_root: "data/chbmit_cache"
  # Available subjects after download (chb04 and others not available)
  subjects: ["chb01", "chb02", "chb03", "chb05", "chb10"]

# =============================================================================
# SIGNAL PROCESSING
# =============================================================================
signal:
  target_sfreq: 256
  bandpass_low: 0.5
  bandpass_high: 45.0
  notch_freq: 60.0
  notch_width: 2.0
  amp_uv_thresh: 500.0

# =============================================================================
# WINDOWING & LABELING
# =============================================================================
windowing:
  window_sec: 10.0
  step_sec: 5.0
  preictal_min: 10
  gap_sec: 30
  postictal_min: 10
  interictal_buffer_min: 30
  tau_sec: 120

# =============================================================================
# FEATURE EXTRACTION
# =============================================================================
features:
  bands:
    delta: [0.5, 4.0]
    theta: [4.0, 8.0]
    alpha: [8.0, 12.0]
    beta: [12.0, 30.0]
    gamma: [30.0, 45.0]
  
  compute_bandpower: true
  compute_ratios: true
  compute_line_length: true
  compute_spectral_entropy: true
  compute_hjorth: true
  compute_kurtosis: true

# =============================================================================
# SPECTROGRAM
# =============================================================================
spectrogram:
  n_fft: 256
  hop_length: 64
  win_length: 256
  log_offset: 1.0

# =============================================================================
# DATA SPLITTING (using available subjects only)
# =============================================================================
split:
  split_mode: "cross_subject"
  
  # Cross-subject: 3 train, 1 val, 1 test (using available subjects)
  train_subjects: ["chb01", "chb02", "chb03"]
  val_subjects: ["chb05"]
  test_subjects: ["chb10"]
  
  # Within-subject settings
  within_subject_id: "chb01"
  within_train_ratio: 0.7
  
  seed: 42

# =============================================================================
# AUGMENTATION (training only)
# =============================================================================
augmentation:
  enabled: true
  
  # Gaussian noise
  gaussian_noise_prob: 0.3
  gaussian_noise_std: 0.05  # Fraction of signal std
  
  # Time shift
  time_shift_prob: 0.3
  time_shift_max_samples: 128  # ~0.5 sec at 256 Hz
  
  # Amplitude scaling
  amplitude_scale_prob: 0.3
  amplitude_scale_range: [0.8, 1.2]
  
  # Channel dropout (random channels zeroed)
  channel_dropout_prob: 0.2
  channel_dropout_max: 3  # Max channels to drop
  
  # SpecAugment-style masking (for spectrogram path)
  spec_freq_mask_prob: 0.2
  spec_freq_mask_width: 10
  spec_time_mask_prob: 0.2
  spec_time_mask_width: 20

# =============================================================================
# BASELINE MODEL
# =============================================================================
baseline:
  model_type: "xgboost"
  xgb_n_estimators: 200
  xgb_max_depth: 5
  xgb_learning_rate: 0.1
  mlp_hidden_dims: [128, 64]
  mlp_dropout: 0.3

# =============================================================================
# DEEP MODEL (regularized)
# =============================================================================
model:
  cnn_channels: [16, 32, 64, 128]
  cnn_kernel_size: 3
  cnn_pool_size: 2
  cnn_embed_dim: 64
  
  feature_hidden_dims: [64, 32]
  feature_embed_dim: 32
  
  fusion_hidden_dims: [64, 32]
  dropout: 0.5  # Increased from 0.3 for regularization
  
  use_features: true

# =============================================================================
# TRAINING (longer, regularized)
# =============================================================================
training:
  epochs: 25
  batch_size: 256  # Good for RTX 3070 with AMP
  learning_rate: 0.001
  weight_decay: 0.001  # Increased from 0.0001 for regularization
  
  scheduler: "cosine"
  scheduler_warmup_epochs: 3
  
  use_amp: true
  gradient_accumulation_steps: 1
  
  early_stopping_patience: 7
  
  lambda_soft: 0.5
  weight_by_proximity: true
  
  # Label smoothing
  label_smoothing: 0.1

# =============================================================================
# EVALUATION
# =============================================================================
evaluation:
  threshold: 0.5
  refractory_min: 20
  threshold_sweep: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
  
  # Target FAH for threshold tuning
  target_fah: [0.2, 0.5, 1.0]

# =============================================================================
# LOGGING
# =============================================================================
logging:
  run_dir: "runs"
  experiment_name: "medium_run"
  log_interval: 20
  save_best_only: true
  save_every_n_epochs: 5

# =============================================================================
# DEMO
# =============================================================================
demo:
  default_threshold: 0.5
  default_speed: 1.0
  channels_to_display: 8
